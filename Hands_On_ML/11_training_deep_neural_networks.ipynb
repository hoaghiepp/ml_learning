{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ba2f66",
   "metadata": {},
   "source": [
    "# The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eeb861",
   "metadata": {},
   "source": [
    "Thuật toán lan truyền ngược (backpropagation) thực hiện bằng cách bắt đầu từ output đến input layer, lan truyền gradient lỗi trên đường đi. Khi tính toán đạo hàm hàm mất mát liên quan đến từng tham số trong mạng, nó dùng đạo hàm để cập nhật các tham số bằng cách giảm gradient.\n",
    "\n",
    "Thật không may, độ dốc (gradient) thường ngày càng nhỏ dần khi thuật toán lan truyền ngược xuống các lớp thấp hơn. Kết quả là, việc cập nhật bằng phương pháp gradient descent gần như không làm thay đổi trọng số kết nối của các lớp thấp, và quá trình huấn luyện không bao giờ hội tụ đến một nghiệm tốt. Hiện tượng này được gọi là vấn đề gradient biến mất (vanishing gradients).\n",
    "\n",
    "Trong một số trường hợp, điều ngược lại có thể xảy ra: gradient có thể ngày càng lớn hơn cho đến khi các lớp nhận được những cập nhật trọng số cực kỳ lớn và thuật toán bị phân kỳ. Đây được gọi là vấn đề gradient bùng nổ (exploding gradients), thường xuất hiện nhất trong các mạng nơ-ron hồi tiếp (recurrent neural networks – xem thêm ở Chương 15).\n",
    "\n",
    "Nói chung, các mạng nơ-ron sâu gặp phải vấn đề gradient không ổn định (unstable gradients); các lớp khác nhau có thể học với tốc độ rất khác nhau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f7705",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
